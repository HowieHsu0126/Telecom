{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import StackingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedGroupKFold\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "# Suppress warnings and RuntimeWarnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "base_models = [\n",
    "    ('xgb_1', XGBClassifier(objective='binary:logistic',\n",
    "     n_estimators=100, n_jobs=-1, random_state=42)),\n",
    "    ('xgb_2', XGBClassifier(objective='binary:logistic',\n",
    "     n_estimators=150, n_jobs=-1, random_state=42)),\n",
    "    # ('xgb_3', XGBClassifier(objective='binary:logistic',\n",
    "    #  n_estimators=200, n_jobs=-1, random_state=42)),\n",
    "    # ('xgb_4', XGBClassifier(objective='binary:logistic',\n",
    "    #  n_estimators=500, n_jobs=-1, random_state=42)),\n",
    "    # ('xgb_5', XGBClassifier(objective='binary:logistic',\n",
    "    #  n_estimators=1000, n_jobs=-1, random_state=42)),\n",
    "]\n",
    "\n",
    "meta_model = XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    objective='binary:logistic',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_data(data, use_msisdn):\n",
    "    logger.info('Preparing data...')\n",
    "    if use_msisdn:\n",
    "        X = data.drop(columns=['is_sa'])\n",
    "    else:\n",
    "        X = data.drop(columns=['is_sa', 'msisdn'])\n",
    "    y = data['is_sa']\n",
    "    groups = data['msisdn']\n",
    "    logger.info('Data preparation completed.')\n",
    "    return X, y, groups\n",
    "\n",
    "\n",
    "def build_stacking_classifier():\n",
    "    logger.info('Building stacking classifier...')\n",
    "    final_estimator = meta_model\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_models,\n",
    "        final_estimator=final_estimator,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    logger.info('Stacking classifier built.')\n",
    "    return stacking_clf\n",
    "\n",
    "\n",
    "def build_voting_classifier():\n",
    "    logger.info('Building voting classifier...')\n",
    "    voting_clf = VotingClassifier(\n",
    "        estimators=base_models,\n",
    "        voting='soft',\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    logger.info('Voting classifier built.')\n",
    "    return voting_clf\n",
    "\n",
    "\n",
    "def build_simple_classifier():\n",
    "    logger.info('Building simple classifier...')\n",
    "    model = base_models[0][1]\n",
    "    logger.info(f'Using model: {model}')\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model(mode='stacking'):\n",
    "    logger.info(f'Building model with mode: {mode}')\n",
    "    if mode == 'stacking':\n",
    "        return build_stacking_classifier()\n",
    "    elif mode == 'simple':\n",
    "        return build_simple_classifier()\n",
    "    elif mode == 'voting':\n",
    "        return build_voting_classifier()\n",
    "    else:\n",
    "        raise ValueError(f'Unknown mode: {mode}')\n",
    "\n",
    "\n",
    "def cross_validate_model(model, X, y, groups, cv_strategy):\n",
    "    logger.info('Starting cross-validation...')\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X, y, cv=cv_strategy, groups=groups, scoring='f1')\n",
    "    logger.info(f'Cross-validation completed. F1 score: {cv_scores.mean()}')\n",
    "    return cv_scores\n",
    "\n",
    "\n",
    "def predict(model, validation_data, output_path, use_msisdn=False):\n",
    "    logger.info('Predicting and saving results...')\n",
    "    msisdn = validation_data['msisdn']\n",
    "    if not use_msisdn:\n",
    "        validation_data = validation_data.drop(columns=['msisdn'])\n",
    "    y_pred = model.predict(validation_data)\n",
    "    result = pd.DataFrame({'msisdn': msisdn, 'is_sa': y_pred})\n",
    "    result.to_csv(output_path, index=False)\n",
    "    logger.info(f'Results saved to {output_path}')\n",
    "\n",
    "\n",
    "def pseudo_labeling(train_data, validation_data, threshold=0.9, mode='voting', use_msisdn=False):\n",
    "    logger.info('Starting pseudo-labeling...')\n",
    "    X_train, y_train, groups_train = prepare_data(train_data, use_msisdn)\n",
    "    X_unlabeled = validation_data.drop(columns=['msisdn']).copy(\n",
    "    ) if not use_msisdn else validation_data.copy()\n",
    "\n",
    "    model = build_model(mode)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_unlabeled_pred = model.predict_proba(X_unlabeled)\n",
    "    high_confidence_indices = (y_unlabeled_pred.max(axis=1) > threshold)\n",
    "\n",
    "    pseudo_labeled_data = validation_data[high_confidence_indices].copy()\n",
    "    pseudo_labeled_data['is_sa'] = y_unlabeled_pred[high_confidence_indices].argmax(\n",
    "        axis=1)\n",
    "\n",
    "    augmented_train_data = pd.concat(\n",
    "        [train_data, pseudo_labeled_data], ignore_index=True)\n",
    "    logger.info(f'Added {pseudo_labeled_data.shape[0]} pseudo-labeled samples.')\n",
    "    logger.info('Pseudo-labeling completed.')\n",
    "    return augmented_train_data\n",
    "\n",
    "\n",
    "def adversarial_validation(train_data, validation_data, use_msisdn=False):\n",
    "    logger.info('Starting adversarial validation...')\n",
    "    train_data['is_train'] = 1\n",
    "    validation_data['is_train'] = 0\n",
    "\n",
    "    combined_data = pd.concat([train_data, validation_data], axis=0)\n",
    "    if use_msisdn:\n",
    "        X = combined_data.drop(columns=['is_sa', 'is_train'])\n",
    "    else:\n",
    "        X = combined_data.drop(columns=['is_sa', 'is_train', 'msisdn'])\n",
    "    y = combined_data['is_train']\n",
    "\n",
    "    adv_model = XGBClassifier(\n",
    "        objective='binary:logistic', n_estimators=100, n_jobs=-1, random_state=42)\n",
    "    adv_model.fit(X, y)\n",
    "\n",
    "    feature_importances = adv_model.feature_importances_\n",
    "    feature_names = X.columns\n",
    "    important_features = [feature for feature, importance in zip(\n",
    "        feature_names, feature_importances) if importance > np.median(feature_importances)]\n",
    "    logger.info(\n",
    "        f'Removed features: {set(feature_names) - set(important_features)}')\n",
    "    logger.info('Adversarial validation completed.')\n",
    "    if use_msisdn:\n",
    "        train_data = train_data[important_features + ['is_sa'] + ['msisdn']]\n",
    "        validation_data = validation_data[important_features + ['msisdn']]\n",
    "    else:\n",
    "        train_data = train_data[important_features + ['is_sa']]\n",
    "        validation_data = validation_data[important_features]\n",
    "    logger.info(\n",
    "        f'Training on {train_data.shape[0]} samples with {train_data.shape[1] - (2 if use_msisdn else 1)} features')\n",
    "    return train_data, validation_data\n",
    "\n",
    "\n",
    "def main(dataset_version, output_path, use_pseudo_labeling=False, threshold=0.95, mode='voting', n_fold=5, need_prediction=False, use_adversarial_validation=False, use_msisdn=False):\n",
    "    logger.info('Main function started.')\n",
    "\n",
    "    train_data = pd.read_csv(\n",
    "        f'/home/hwxu/Projects/Competition/Telecom/Input/processed/train{dataset_version}.csv')\n",
    "    validation_data = pd.read_csv(\n",
    "        f'/home/hwxu/Projects/Competition/Telecom/Input/processed/val{dataset_version}.csv')\n",
    "    logger.info(\n",
    "        f\"Training on {train_data.shape[0]} samples with {validation_data.shape[1]} features.\")\n",
    "\n",
    "    if use_adversarial_validation:\n",
    "        train_data, validation_data = adversarial_validation(\n",
    "            train_data, validation_data, use_msisdn)\n",
    "    else:\n",
    "        train_data = train_data\n",
    "        validation_data = validation_data\n",
    "\n",
    "    if use_pseudo_labeling:\n",
    "        train_data = pseudo_labeling(\n",
    "            train_data, validation_data, threshold, mode, use_msisdn)\n",
    "    else:\n",
    "        train_data = train_data\n",
    "\n",
    "    X, y, groups = prepare_data(train_data, use_msisdn)\n",
    "    cv_strategy = StratifiedGroupKFold(n_splits=n_fold)\n",
    "\n",
    "    classifier = build_model(mode)\n",
    "\n",
    "    cross_validate_model(classifier, X, y, groups, cv_strategy)\n",
    "\n",
    "    if need_prediction:\n",
    "        classifier.fit(X, y)\n",
    "        predict(classifier, validation_data, output_path, use_msisdn)\n",
    "    logger.info('Main function completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-27 05:12:29,390 - INFO - Main function started.\n",
      "2024-07-27 05:12:29,546 - INFO - Training on 3836 samples with 61 features.\n",
      "2024-07-27 05:12:29,547 - INFO - Starting pseudo-labeling...\n",
      "2024-07-27 05:12:29,547 - INFO - Preparing data...\n",
      "2024-07-27 05:12:29,551 - INFO - Data preparation completed.\n",
      "2024-07-27 05:12:29,552 - INFO - Building model with mode: simple\n",
      "2024-07-27 05:12:29,553 - INFO - Building simple classifier...\n",
      "2024-07-27 05:12:29,556 - INFO - Using model: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=100, n_jobs=-1,\n",
      "              num_parallel_tree=None, random_state=42, ...)\n",
      "2024-07-27 05:12:31,125 - INFO - Added 1112 pseudo-labeled samples.\n",
      "2024-07-27 05:12:31,127 - INFO - Pseudo-labeling completed.\n",
      "2024-07-27 05:12:31,128 - INFO - Preparing data...\n",
      "2024-07-27 05:12:31,131 - INFO - Data preparation completed.\n",
      "2024-07-27 05:12:31,133 - INFO - Building model with mode: simple\n",
      "2024-07-27 05:12:31,133 - INFO - Building simple classifier...\n",
      "2024-07-27 05:12:31,135 - INFO - Using model: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=100, n_jobs=-1,\n",
      "              num_parallel_tree=None, random_state=42, ...)\n",
      "2024-07-27 05:12:31,136 - INFO - Starting cross-validation...\n",
      "2024-07-27 05:12:54,397 - INFO - Cross-validation completed. F1 score: 0.8141340388119197\n",
      "2024-07-27 05:12:54,399 - INFO - Main function completed.\n"
     ]
    }
   ],
   "source": [
    "dataset_version = 60\n",
    "output_path = f'/home/hwxu/Projects/Competition/Telecom/Output/submissions/pred-{dataset_version}_{datetime.now()}.csv'\n",
    "\n",
    "main(\n",
    "    dataset_version,\n",
    "    output_path,\n",
    "    need_prediction=False,\n",
    "    use_pseudo_labeling=True,\n",
    "    use_adversarial_validation=False,\n",
    "    use_msisdn=True,\n",
    "    threshold=0.95,\n",
    "    mode='simple',\n",
    "    n_fold=10,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hwxu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
