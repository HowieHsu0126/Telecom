{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE, SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from category_encoders import HashingEncoder, CountEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DatetimeConvertCyclical(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.time_periods = {\n",
    "            'second': 24 * 60 * 60,\n",
    "            'minute': 24 * 60,\n",
    "            'hour': 24,\n",
    "            'day': 30,\n",
    "            'dayofweek': 7,\n",
    "            'month': 12\n",
    "        }\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        for period, value in self.time_periods.items():\n",
    "            X[period] = getattr(X['timestamp'].dt, period)\n",
    "            X[f'sin_{period}'] = np.sin(2 * np.pi * X[period] / value)\n",
    "            X[f'cos_{period}'] = np.cos(2 * np.pi * X[period] / value)\n",
    "            X.drop(period, axis=1, inplace=True)\n",
    "        return X\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self):\n",
    "        self.encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_time_columns(data):\n",
    "        data['start_time'] = pd.to_datetime(data['start_time'], format='%Y%m%d%H%M%S')\n",
    "        data['end_time'] = pd.to_datetime(data['end_time'], format='%Y%m%d%H%M%S')\n",
    "        data['timestamp'] = data['start_time']\n",
    "        return data\n",
    "\n",
    "    def _extract_time_features(self, data):\n",
    "        cyclical_transformer = DatetimeConvertCyclical()\n",
    "        data = cyclical_transformer.transform(data)\n",
    "        data['start_hour'] = data['start_time'].dt.hour\n",
    "        data['start_dayofweek'] = data['start_time'].dt.dayofweek\n",
    "        data['is_weekend'] = data['start_dayofweek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "        data['is_working_hour'] = data['start_hour'].apply(lambda x: 1 if 9 <= x <= 18 else 0)\n",
    "        return data\n",
    "\n",
    "    def _encode_feature(self, data, feature, encoding_method, fit=True):\n",
    "        if encoding_method == 'onehot':\n",
    "            encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') if fit else self.encoders.get(feature)\n",
    "            if fit:\n",
    "                encoded = encoder.fit_transform(data[[feature]])\n",
    "                self.encoders[feature] = encoder\n",
    "            else:\n",
    "                encoded = encoder.transform(data[[feature]])\n",
    "            encoded_df = pd.DataFrame(encoded, columns=[f\"{feature}_{cat}\" for cat in encoder.categories_[0]])\n",
    "            data = pd.concat([data, encoded_df], axis=1).drop(columns=[feature])\n",
    "        elif encoding_method == 'label':\n",
    "            encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1) if fit else self.encoders.get(feature)\n",
    "            if fit:\n",
    "                data[feature] = encoder.fit_transform(data[[feature]])\n",
    "                self.encoders[feature] = encoder\n",
    "            else:\n",
    "                data[feature] = encoder.transform(data[[feature]])\n",
    "        elif encoding_method == 'hash':\n",
    "            encoder = HashingEncoder() if fit else self.encoders.get(feature)\n",
    "            if fit:\n",
    "                data = pd.concat([data, encoder.fit_transform(data[[feature]])], axis=1)\n",
    "                self.encoders[feature] = encoder\n",
    "            else:\n",
    "                data = pd.concat([data, encoder.transform(data[[feature]])], axis=1)\n",
    "        elif encoding_method == 'count':\n",
    "            encoder = CountEncoder() if fit else self.encoders.get(feature)\n",
    "            if fit:\n",
    "                data[feature] = encoder.fit_transform(data[[feature]])\n",
    "                self.encoders[feature] = encoder\n",
    "            else:\n",
    "                data[feature] = encoder.transform(data[[feature]]) if encoder else data[feature].map(data[feature].value_counts())\n",
    "        elif encoding_method == 'frequency':\n",
    "            freq = data[feature].value_counts() / len(data) if fit else self.encoders.get(feature, data[feature].value_counts() / len(data))\n",
    "            data[feature] = data[feature].map(freq).fillna(0)\n",
    "            if fit:\n",
    "                self.encoders[feature] = freq\n",
    "        elif encoding_method == 'labelcount':\n",
    "            encoder = CountEncoder(normalize=True) if fit else self.encoders.get(feature)\n",
    "            if fit:\n",
    "                data[feature] = encoder.fit_transform(data[[feature]])\n",
    "                self.encoders[feature] = encoder\n",
    "            else:\n",
    "                data[feature] = encoder.transform(data[[feature]])\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown encoding method: {encoding_method}\")\n",
    "        return data\n",
    "\n",
    "    def _encode_categorical_features(self, data, encoding_config, fit=True):\n",
    "        for feature, encoding_method in encoding_config.items():\n",
    "            data = self._encode_feature(data, feature, encoding_method, fit)\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def _statistical_features(df, feature):\n",
    "        agg_funcs = ['sum', 'mean', 'max', 'min', 'std', 'var', 'median', 'nunique', 'size', 'count']\n",
    "        agg_funcs += [\n",
    "            ('skew', lambda x: skew(x) if len(x) > 1 else -1),\n",
    "            ('kurt', lambda x: kurtosis(x) if len(x) > 1 else -1),\n",
    "            ('quantile_25', lambda x: x.quantile(0.25)),\n",
    "            ('quantile_75', lambda x: x.quantile(0.75)),\n",
    "            ('mode', lambda x: x.mode().iloc[0] if not x.mode().empty else -1)\n",
    "        ]\n",
    "        return df.groupby('msisdn')[feature].agg(agg_funcs).add_prefix(f'{feature}_')\n",
    "\n",
    "    @staticmethod\n",
    "    def _aggregate_features(data, features, agg_func):\n",
    "        user_aggregated_data = pd.DataFrame()\n",
    "        for feature in features:\n",
    "            feature_stats = agg_func(data, feature)\n",
    "            if user_aggregated_data.empty:\n",
    "                user_aggregated_data = feature_stats\n",
    "            else:\n",
    "                user_aggregated_data = user_aggregated_data.join(feature_stats, how='outer')\n",
    "        return user_aggregated_data\n",
    "\n",
    "    def _aggregate_numerical_features(self, data, numerical_features):\n",
    "        return self._aggregate_features(data, numerical_features, self._statistical_features)\n",
    "\n",
    "    @staticmethod\n",
    "    def _aggregate_location_features(data, location_features):\n",
    "        return data.groupby('msisdn')[location_features].agg(['nunique', 'count']).add_prefix('_')\n",
    "\n",
    "    @staticmethod\n",
    "    def _aggregate_categorical_frequencies(data, categorical_features, user_aggregated_data):\n",
    "        for feature in categorical_features:\n",
    "            frequency = data.groupby(['msisdn', feature]).size().unstack(fill_value=0)\n",
    "            normalized_frequency = frequency.div(frequency.sum(axis=1), axis=0)\n",
    "            normalized_frequency.columns = [f\"{feature}_{col}_freq\" for col in normalized_frequency.columns]\n",
    "            user_aggregated_data = user_aggregated_data.join(normalized_frequency, how='left')\n",
    "        return user_aggregated_data\n",
    "\n",
    "    @staticmethod\n",
    "    def _aggregate_differential_features(data):\n",
    "        data['call_duration_diff'] = data.groupby('msisdn')['call_duration'].diff().fillna(0)\n",
    "        data['call_duration_diff2'] = data.groupby('msisdn')['call_duration'].diff(2).fillna(0)\n",
    "\n",
    "        diff_agg_funcs = {\n",
    "            'call_duration_diff': ['mean', 'std'],\n",
    "            'call_duration_diff2': ['mean', 'std']\n",
    "        }\n",
    "\n",
    "        diff_aggregated_data = data.groupby('msisdn').agg(diff_agg_funcs)\n",
    "        diff_aggregated_data.columns = ['_'.join(map(str, col)).strip() for col in diff_aggregated_data.columns.values]\n",
    "\n",
    "        return diff_aggregated_data\n",
    "\n",
    "    def _extract_advanced_features(self, data):\n",
    "        call_stats = data.groupby('msisdn').agg({\n",
    "            'msisdn': ['count', 'nunique']\n",
    "        })\n",
    "        call_stats.columns = ['_'.join(col).strip() for col in call_stats.columns.values]\n",
    "        call_stats['avg_calls_per_person'] = call_stats['msisdn_count'] / call_stats['msisdn_nunique']\n",
    "\n",
    "        most_common_city = data.groupby('msisdn')['phone1_loc_city'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else -1)\n",
    "        most_common_county = data.groupby('msisdn')['phone1_loc_province'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else -1)\n",
    "\n",
    "        data['date'] = data['start_time'].dt.date\n",
    "        daily_stats = data.groupby(['msisdn', 'date']).agg({\n",
    "            'start_hour': ['min', 'max'],\n",
    "            'call_duration': 'count'\n",
    "        })\n",
    "        daily_stats.columns = ['_'.join(col).strip() for col in daily_stats.columns.values]\n",
    "        daily_stats.reset_index(inplace=True)\n",
    "\n",
    "        daily_stats_agg = daily_stats.groupby('msisdn').agg({\n",
    "            'start_hour_min': ['mean', 'std'],\n",
    "            'start_hour_max': ['mean', 'std'],\n",
    "            'call_duration_count': ['mean', 'std']\n",
    "        })\n",
    "        daily_stats_agg.columns = ['_'.join(col).strip() for col in daily_stats_agg.columns.values]\n",
    "\n",
    "        advanced_features = pd.concat([call_stats, most_common_city, most_common_county, daily_stats_agg], axis=1)\n",
    "        advanced_features.columns = [f\"advanced_{col}\" for col in advanced_features.columns]\n",
    "\n",
    "        return advanced_features\n",
    "\n",
    "    @staticmethod\n",
    "    def _binary_operations(user_aggregated_data):\n",
    "        if 'cfee_sum' in user_aggregated_data.columns and 'lfee_sum' in user_aggregated_data.columns:\n",
    "            user_aggregated_data['cfee_lfee_sum'] = user_aggregated_data['cfee_sum'] + user_aggregated_data['lfee_sum']\n",
    "            user_aggregated_data['cfee_lfee_diff'] = user_aggregated_data['cfee_sum'] - user_aggregated_data['lfee_sum']\n",
    "            user_aggregated_data['cfee_lfee_prod'] = user_aggregated_data['cfee_sum'] * user_aggregated_data['lfee_sum']\n",
    "            user_aggregated_data['cfee_lfee_ratio'] = user_aggregated_data['cfee_sum'] / (user_aggregated_data['lfee_sum'] + 1e-6)\n",
    "        return user_aggregated_data\n",
    "\n",
    "    def _add_ranking_features(self, data, features):\n",
    "        for feature in features:\n",
    "            data[f'{feature}_rank'] = data.groupby('msisdn')[feature].rank(method='average')\n",
    "            data[f'{feature}_dense_rank'] = data.groupby('msisdn')[feature].rank(method='dense')\n",
    "        return data\n",
    "\n",
    "    def preprocess_and_aggregate(self, data, label_data=None, is_validation=False, fit_columns=None, encoding_config=None):\n",
    "        data = self._convert_time_columns(data)\n",
    "        data = self._extract_time_features(data)\n",
    "\n",
    "        suspect_types = {3, 5, 6, 9, 11, 12, 17}\n",
    "        data['is_suspect'] = data['phone1_type'].apply(lambda x: 1 if x in suspect_types else 0)\n",
    "\n",
    "        categorical_features = [\n",
    "            'call_event', 'roam_type', 'long_type1', 'ismultimedia', 'home_area_code',\n",
    "            'visit_area_code', 'called_home_code', 'called_code', 'a_serv_type',\n",
    "            'a_product_id', 'phone1_type', 'phone2_type', 'phone1_loc_city',\n",
    "            'phone1_loc_province', 'phone2_loc_city', 'phone2_loc_province',\n",
    "            'is_weekend', 'is_working_hour'\n",
    "        ]\n",
    "        location_features = [\n",
    "            'home_area_code', 'visit_area_code', 'called_home_code', 'called_code',\n",
    "            'phone1_loc_city', 'phone1_loc_province', 'phone2_loc_city', 'phone2_loc_province'\n",
    "        ]\n",
    "        numerical_features = ['call_duration', 'cfee', 'lfee', 'start_hour', 'start_dayofweek']\n",
    "\n",
    "        data = self._encode_categorical_features(data, encoding_config, fit=not is_validation)\n",
    "\n",
    "        # Include the new features in the aggregation\n",
    "        all_numerical_features = numerical_features + [f'sin_{period}' for period in ['hour', 'dayofweek']] + [f'cos_{period}' for period in ['hour', 'dayofweek']]\n",
    "        \n",
    "        user_aggregated_data = self._aggregate_numerical_features(data, all_numerical_features)\n",
    "        user_aggregated_data = user_aggregated_data.join(self._aggregate_location_features(data, location_features), how='outer')\n",
    "        user_aggregated_data = self._aggregate_categorical_frequencies(data, categorical_features, user_aggregated_data)\n",
    "        user_aggregated_data = user_aggregated_data.join(self._aggregate_differential_features(data), how='left')\n",
    "\n",
    "        user_aggregated_data = self._binary_operations(user_aggregated_data)\n",
    "        advanced_features = self._extract_advanced_features(data)\n",
    "        user_aggregated_data = user_aggregated_data.join(advanced_features, how='left')\n",
    "\n",
    "        ranking_features = ['call_duration', 'cfee', 'lfee'] + [f'sin_{period}' for period in ['hour', 'dayofweek']] + [f'cos_{period}' for period in ['hour', 'dayofweek']]\n",
    "        user_aggregated_data = self._add_ranking_features(user_aggregated_data, ranking_features)\n",
    "\n",
    "        user_aggregated_data.fillna(-1, inplace=True)\n",
    "        user_aggregated_data.reset_index(inplace=True)\n",
    "\n",
    "        if not is_validation and label_data is not None:\n",
    "            user_aggregated_data = user_aggregated_data.merge(label_data, on='msisdn', how='left')\n",
    "\n",
    "        if not is_validation:\n",
    "            numerical_features = [col for col in user_aggregated_data.columns if col not in ['msisdn', 'is_sa']]\n",
    "            user_aggregated_data[numerical_features] = self.scaler.fit_transform(user_aggregated_data[numerical_features])\n",
    "            return user_aggregated_data, numerical_features, self.scaler\n",
    "        else:\n",
    "            user_aggregated_data[fit_columns] = self.scaler.transform(user_aggregated_data[fit_columns])\n",
    "            return user_aggregated_data\n",
    "\n",
    "def feature_selection(train_data, label_column='is_sa', k=20):\n",
    "    X = train_data.drop(columns=['msisdn', label_column])\n",
    "    y = train_data[label_column]\n",
    "\n",
    "    def get_top_features(selector):\n",
    "        selector.fit(X, y)\n",
    "        return X.columns[selector.get_support()].tolist()\n",
    "\n",
    "    xgb_model = XGBClassifier(n_jobs=-1)\n",
    "    xgb_selected_features = get_top_features(xgb_model)\n",
    "\n",
    "    mutual_info_selected_features = get_top_features(SelectKBest(mutual_info_classif, k=k))\n",
    "\n",
    "    rfe_model = RFE(LogisticRegression(max_iter=1000), n_features_to_select=k)\n",
    "    rfe_selected_features = get_top_features(rfe_model)\n",
    "\n",
    "    info_gain_selected_features = get_top_features(SelectKBest(f_classif, k=k))\n",
    "\n",
    "    corr_coef_selected_features = X.corrwith(y).abs().nlargest(k).index.tolist()\n",
    "\n",
    "    all_selected_features = (\n",
    "        xgb_selected_features + mutual_info_selected_features +\n",
    "        rfe_selected_features + info_gain_selected_features +\n",
    "        corr_coef_selected_features + rf_selected_features\n",
    "    )\n",
    "\n",
    "    combined_features = pd.Series(all_selected_features).value_counts().nlargest(k).index.tolist()\n",
    "\n",
    "    return combined_features\n",
    "\n",
    "# Encoding configuration with Count Encoding for location features\n",
    "encoding_config = {\n",
    "    'call_event': 'label',\n",
    "    'other_party': 'label',\n",
    "    'ismultimedia': 'label',\n",
    "    'home_area_code': 'frequency',\n",
    "    'visit_area_code': 'frequency',\n",
    "    'called_home_code': 'frequency',\n",
    "    'called_code': 'frequency',\n",
    "    'a_serv_type': 'onehot',\n",
    "    'long_type1': 'label',\n",
    "    'roam_type': 'label',\n",
    "    'a_product_id': 'count',\n",
    "    'phone1_type': 'label',\n",
    "    'phone2_type': 'label',\n",
    "    'phone1_loc_city': 'frequency',\n",
    "    'phone1_loc_province': 'frequency',\n",
    "    'phone2_loc_city': 'frequency',\n",
    "    'phone2_loc_province': 'frequency',\n",
    "    'is_weekend': 'label',\n",
    "    'is_working_hour': 'label'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "train_set_res = pd.read_csv(\n",
    "    '/home/hwxu/Projects/Competition/Telecom/Input/raw/train.csv', low_memory=False)\n",
    "train_set_ans = pd.read_csv(\n",
    "    '/home/hwxu/Projects/Competition/Telecom/Input/raw/labels.csv', low_memory=False)\n",
    "validation_set_res = pd.read_csv(\n",
    "    '/home/hwxu/Projects/Competition/Telecom/Input/raw/val.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3200653/1393382626.py:146: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  ('skew', lambda x: skew(x) if len(x) > 1 else -1),\n",
      "/tmp/ipykernel_3200653/1393382626.py:147: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  ('kurt', lambda x: kurtosis(x) if len(x) > 1 else -1),\n",
      "/tmp/ipykernel_3200653/1393382626.py:146: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  ('skew', lambda x: skew(x) if len(x) > 1 else -1),\n",
      "/tmp/ipykernel_3200653/1393382626.py:147: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  ('kurt', lambda x: kurtosis(x) if len(x) > 1 else -1),\n",
      "/tmp/ipykernel_3200653/1393382626.py:146: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  ('skew', lambda x: skew(x) if len(x) > 1 else -1),\n",
      "/tmp/ipykernel_3200653/1393382626.py:147: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  ('kurt', lambda x: kurtosis(x) if len(x) > 1 else -1),\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Column not found: call_duration'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m data_processor \u001b[38;5;241m=\u001b[39m DataProcessor()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 处理训练集\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_data, fit_columns, scaler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_and_aggregate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set_ans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(fit_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 选择特征\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 316\u001b[0m, in \u001b[0;36mDataProcessor.preprocess_and_aggregate\u001b[0;34m(self, data, label_data, is_validation, fit_columns, encoding_config)\u001b[0m\n\u001b[1;32m    312\u001b[0m user_aggregated_data \u001b[38;5;241m=\u001b[39m user_aggregated_data\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    313\u001b[0m     advanced_features, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    315\u001b[0m ranking_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall_duration\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcfee\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlfee\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 316\u001b[0m user_aggregated_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_ranking_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_aggregated_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranking_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m user_aggregated_data\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    319\u001b[0m user_aggregated_data\u001b[38;5;241m.\u001b[39mreset_index(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[13], line 272\u001b[0m, in \u001b[0;36mDataProcessor._add_ranking_features\u001b[0;34m(self, data, features)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_add_ranking_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, features):\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m--> 272\u001b[0m         data[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_rank\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmsisdn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mrank(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    273\u001b[0m         data[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_dense_rank\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmsisdn\u001b[39m\u001b[38;5;124m'\u001b[39m)[feature]\u001b[38;5;241m.\u001b[39mrank(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdense\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/hw/lib/python3.9/site-packages/pandas/core/groupby/generic.py:1964\u001b[0m, in \u001b[0;36mDataFrameGroupBy.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;66;03m# if len == 1, then it becomes a SeriesGroupBy and this is actually\u001b[39;00m\n\u001b[1;32m   1959\u001b[0m     \u001b[38;5;66;03m# valid syntax, so don't raise\u001b[39;00m\n\u001b[1;32m   1960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1961\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot subset columns with a tuple with more than one element. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1962\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a list instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1963\u001b[0m     )\n\u001b[0;32m-> 1964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hw/lib/python3.9/site-packages/pandas/core/base.py:244\u001b[0m, in \u001b[0;36mSelectionMixin.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    245\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj[key]\u001b[38;5;241m.\u001b[39mndim\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gotitem(key, ndim\u001b[38;5;241m=\u001b[39mndim)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Column not found: call_duration'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 实例化数据处理器\n",
    "data_processor = DataProcessor()\n",
    "\n",
    "# 处理训练集\n",
    "train_data, fit_columns, scaler = data_processor.preprocess_and_aggregate(\n",
    "    train_set_res, train_set_ans, is_validation=False, encoding_config=encoding_config)\n",
    "\n",
    "print(f\"num features: {len(fit_columns)}\")\n",
    "\n",
    "# 选择特征\n",
    "n_features_to_keep = 30\n",
    "selected_features = feature_selection(train_data, k=n_features_to_keep)\n",
    "train_data = train_data[['msisdn'] + selected_features + ['is_sa']]\n",
    "\n",
    "print(f\"num selected features: {len(selected_features)}\")\n",
    "\n",
    "# 处理验证集\n",
    "validation_data = data_processor.preprocess_and_aggregate(\n",
    "    validation_set_res, is_validation=True, fit_columns=fit_columns, encoding_config=encoding_config)\n",
    "\n",
    "# 选择特征（根据训练集选择的特征）\n",
    "validation_data = validation_data[['msisdn'] + selected_features]\n",
    "\n",
    "# 输出处理后的训练集和验证集\n",
    "train_data.to_csv(\n",
    "    f'/home/hwxu/Projects/Competition/Telecom/Input/processed/train{n_features_to_keep}.csv', index=False)\n",
    "validation_data.to_csv(\n",
    "    f'/home/hwxu/Projects/Competition/Telecom/Input/processed/val{n_features_to_keep}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, validation_data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hwxu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
