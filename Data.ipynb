{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE, SelectKBest, SelectFromModel, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from category_encoders import HashingEncoder, CountEncoder\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "\n",
    "# Suppress warnings and RuntimeWarnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class DatetimeConvertCyclical(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.time_periods = {'second': 24 * 60 * 60,\n",
    "                             'minute': 24 * 60,\n",
    "                             'hour': 24,\n",
    "                             'day': 30,\n",
    "                             'dayofweek': 7,\n",
    "                             'month': 12}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, time_col='timestamp'):\n",
    "        for period, value in self.time_periods.items():\n",
    "            X[period] = getattr(X[time_col].dt, period)\n",
    "\n",
    "            X[f'{time_col}_sin_' +\n",
    "                period] = np.sin(2 * np.pi * X[period] / value)\n",
    "            X[f'{time_col}_cos_' +\n",
    "                period] = np.cos(2 * np.pi * X[period] / value)\n",
    "\n",
    "            X.drop(str(period), axis=1, inplace=True)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self):\n",
    "        self.encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_time_columns(data):\n",
    "        data['start_time'] = pd.to_datetime(\n",
    "            data['start_time'], format='%Y%m%d%H%M%S')\n",
    "        data['end_time'] = pd.to_datetime(\n",
    "            data['end_time'], format='%Y%m%d%H%M%S')\n",
    "        data['update_time'] = pd.to_datetime(\n",
    "            data['update_time'], format='%Y-%m-%d %H:%M:%S')\n",
    "        data['date'] = pd.to_datetime(\n",
    "            data['date'], format='%Y-%m-%d')\n",
    "        data['date_c'] = pd.to_datetime(\n",
    "            data['date_c'], format='%Y%m%d')\n",
    "        return data\n",
    "\n",
    "    def _extract_time_features(self, data):\n",
    "        cyclical_transformer = DatetimeConvertCyclical()\n",
    "        data = cyclical_transformer.transform(data, 'date')\n",
    "        data['start_hour'] = data['start_time'].dt.hour\n",
    "        data['start_dayofweek'] = data['start_time'].dt.dayofweek\n",
    "        data['is_weekend'] = data['start_dayofweek'].apply(\n",
    "            lambda x: 1 if x >= 5 else 0)\n",
    "        data['is_working_hour'] = data['start_hour'].apply(\n",
    "            lambda x: 1 if 9 <= x <= 18 else 0)\n",
    "        return data\n",
    "\n",
    "    def _encode_categorical_features(self, data, encoding_config, fit=True):\n",
    "        for feature, encoding_method in encoding_config.items():\n",
    "            if encoding_method == 'onehot':\n",
    "                if fit:\n",
    "                    encoder = OneHotEncoder(\n",
    "                        sparse_output=False, handle_unknown='ignore')\n",
    "                    encoded = encoder.fit_transform(data[[feature]])\n",
    "                    self.encoders[feature] = encoder\n",
    "                else:\n",
    "                    encoder = self.encoders.get(feature)\n",
    "                    if encoder:\n",
    "                        encoded = encoder.transform(data[[feature]])\n",
    "                    else:\n",
    "                        continue  # Skip if encoder is not available\n",
    "                encoded_df = pd.DataFrame(\n",
    "                    encoded, columns=[f\"{feature}_{cat}\" for cat in encoder.categories_[0]])\n",
    "                data = pd.concat([data, encoded_df], axis=1)\n",
    "                data.drop(columns=[feature], inplace=True)\n",
    "            elif encoding_method == 'label':\n",
    "                if fit:\n",
    "                    encoder = OrdinalEncoder(\n",
    "                        handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "                    encoded = encoder.fit_transform(data[[feature]])\n",
    "                    self.encoders[feature] = encoder\n",
    "                else:\n",
    "                    encoder = self.encoders.get(feature)\n",
    "                    if encoder:\n",
    "                        encoded = encoder.transform(data[[feature]])\n",
    "                    else:\n",
    "                        continue  # Skip if encoder is not available\n",
    "                data[feature] = encoded\n",
    "            elif encoding_method == 'hash':\n",
    "                if fit:\n",
    "                    encoder = HashingEncoder()\n",
    "                    encoded = encoder.fit_transform(data[[feature]])\n",
    "                    self.encoders[feature] = encoder\n",
    "                else:\n",
    "                    encoder = self.encoders.get(feature)\n",
    "                    if encoder:\n",
    "                        encoded = encoder.transform(data[[feature]])\n",
    "                    else:\n",
    "                        continue  # Skip if encoder is not available\n",
    "                data = pd.concat([data, encoded], axis=1)\n",
    "            elif encoding_method == 'count':\n",
    "                if fit:\n",
    "                    encoder = CountEncoder()\n",
    "                    encoded = encoder.fit_transform(data[[feature]])\n",
    "                    self.encoders[feature] = encoder\n",
    "                else:\n",
    "                    encoder = self.encoders.get(feature)\n",
    "                    if encoder:\n",
    "                        encoded = encoder.transform(data[[feature]])\n",
    "                    else:\n",
    "                        encoded = data[feature].map(\n",
    "                            data[feature].value_counts())  # Frequency encoding\n",
    "                data[feature] = encoded\n",
    "            elif encoding_method == 'frequency':\n",
    "                if fit:\n",
    "                    freq = data[feature].value_counts() / len(data)\n",
    "                    self.encoders[feature] = freq\n",
    "                else:\n",
    "                    freq = self.encoders.get(\n",
    "                        feature, data[feature].value_counts() / len(data))\n",
    "                data[feature] = data[feature].map(freq).fillna(0)\n",
    "            elif encoding_method == 'labelcount':\n",
    "                if fit:\n",
    "                    encoder = CountEncoder(normalize=True)\n",
    "                    encoded = encoder.fit_transform(data[[feature]])\n",
    "                    self.encoders[feature] = encoder\n",
    "                else:\n",
    "                    encoder = self.encoders.get(feature)\n",
    "                    if encoder:\n",
    "                        encoded = encoder.transform(data[[feature]])\n",
    "                    else:\n",
    "                        continue  # Skip if encoder is not available\n",
    "                data[feature] = encoded\n",
    "            elif encoding_method == 'tfidf':\n",
    "                vectorizer = self.tfidf_vectorizers.get(\n",
    "                    feature, TfidfVectorizer())\n",
    "                if fit:\n",
    "                    encoded = vectorizer.fit_transform(\n",
    "                        data[feature].astype(str)).toarray()\n",
    "                    self.tfidf_vectorizers[feature] = vectorizer\n",
    "                else:\n",
    "                    encoded = vectorizer.transform(\n",
    "                        data[feature].astype(str)).toarray()\n",
    "                encoded_df = pd.DataFrame(\n",
    "                    encoded, columns=[f\"tfidf_{feature}_{i}\" for i in range(encoded.shape[1])])\n",
    "                data = pd.concat([data, encoded_df], axis=1).drop(\n",
    "                    columns=[feature])\n",
    "                continue\n",
    "            elif encoding_method == 'countvector':\n",
    "                vectorizer = self.count_vectorizers.get(\n",
    "                    feature, CountVectorizer())\n",
    "                if fit:\n",
    "                    encoded = vectorizer.fit_transform(\n",
    "                        data[feature].astype(str)).toarray()\n",
    "                    self.count_vectorizers[feature] = vectorizer\n",
    "                else:\n",
    "                    encoded = vectorizer.transform(\n",
    "                        data[feature].astype(str)).toarray()\n",
    "                encoded_df = pd.DataFrame(\n",
    "                    encoded, columns=[f\"{feature}_count_{i}\" for i in range(encoded.shape[1])])\n",
    "                data = pd.concat([data, encoded_df], axis=1).drop(\n",
    "                    columns=[feature])\n",
    "                continue\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown encoding method: {encoding_method}\")\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def _statistical_features(df, feature):\n",
    "        agg_funcs = ['sum', 'mean', 'max', 'min', 'std',\n",
    "                     'var', 'median', 'nunique', 'size', 'count']\n",
    "        agg_funcs += [\n",
    "            ('skew', lambda x: skew(x) if len(x) > 1 else np.nan),\n",
    "            ('kurt', lambda x: kurtosis(x) if len(x) > 1 else np.nan),\n",
    "            ('quantile_25', lambda x: x.quantile(0.25)),\n",
    "            ('quantile_75', lambda x: x.quantile(0.75)),\n",
    "            ('mode', lambda x: x.mode().iloc[0]\n",
    "             if not x.mode().empty else np.nan)\n",
    "        ]\n",
    "        return df.groupby('msisdn')[feature].agg(agg_funcs).add_prefix(f'{feature}_')\n",
    "\n",
    "    @staticmethod\n",
    "    def _aggregate_numerical_features(data, numerical_features):\n",
    "        user_aggregated_data = pd.DataFrame()\n",
    "        for feature in numerical_features:\n",
    "            feature_stats = DataProcessor._statistical_features(data, feature)\n",
    "            if user_aggregated_data.empty:\n",
    "                user_aggregated_data = feature_stats\n",
    "            else:\n",
    "                user_aggregated_data = user_aggregated_data.join(\n",
    "                    feature_stats, how='outer')\n",
    "        return user_aggregated_data\n",
    "\n",
    "    @staticmethod\n",
    "    def _aggregate_location_features(data, location_features):\n",
    "        user_aggregated_data = pd.DataFrame()\n",
    "        for feature in location_features:\n",
    "            feature_stats = data.groupby(\n",
    "                'msisdn')[feature].agg(['nunique', 'count'])\n",
    "            feature_stats.columns = [\n",
    "                f'{feature}_{stat}' for stat in feature_stats.columns]\n",
    "            if user_aggregated_data.empty:\n",
    "                user_aggregated_data = feature_stats\n",
    "            else:\n",
    "                user_aggregated_data = user_aggregated_data.join(\n",
    "                    feature_stats, how='outer')\n",
    "        return user_aggregated_data\n",
    "\n",
    "    @staticmethod\n",
    "    def _aggregate_categorical_frequencies(data, categorical_features, user_aggregated_data):\n",
    "\n",
    "        for feature in categorical_features:\n",
    "            frequency = data.groupby(\n",
    "                ['msisdn', feature]).size().unstack(fill_value=0)\n",
    "            normalized_frequency = frequency.div(frequency.sum(axis=1), axis=0)\n",
    "            normalized_frequency.columns = [\n",
    "                f\"{feature}_{col}_freq\" for col in normalized_frequency.columns]\n",
    "            user_aggregated_data = user_aggregated_data.join(\n",
    "                normalized_frequency, how='left')\n",
    "        return user_aggregated_data\n",
    "\n",
    "    @staticmethod\n",
    "    def _aggregate_differential_features(data, numeric_features):\n",
    "        for feature in numeric_features:\n",
    "            data[f'{feature}_diff'] = data.groupby(\n",
    "                'msisdn')[feature].diff().fillna(0)\n",
    "            data[f'{feature}_diff2'] = data.groupby(\n",
    "                'msisdn')[feature].diff(2).fillna(0)\n",
    "\n",
    "        diff_agg_funcs = {f'{feature}_diff': [\n",
    "            'mean', 'std'] for feature in numeric_features}\n",
    "        diff_agg_funcs.update(\n",
    "            {f'{feature}_diff2': ['mean', 'std'] for feature in numeric_features})\n",
    "\n",
    "        diff_aggregated_data = data.groupby('msisdn').agg(diff_agg_funcs)\n",
    "        diff_aggregated_data.columns = [\n",
    "            '_'.join(map(str, col)).strip() for col in diff_aggregated_data.columns.values]\n",
    "\n",
    "        return diff_aggregated_data\n",
    "\n",
    "    def _extract_advanced_features(self, data):\n",
    "\n",
    "        call_stats = data.groupby('msisdn').agg({\n",
    "            'msisdn': ['count', 'nunique']\n",
    "        })\n",
    "        call_stats.columns = ['_'.join(col).strip()\n",
    "                              for col in call_stats.columns.values]\n",
    "        call_stats['avg_calls_per_person'] = call_stats['msisdn_count'] / \\\n",
    "            call_stats['msisdn_nunique']\n",
    "\n",
    "        # 每个号码的通话最多的地市和区县\n",
    "        most_common_city = data.groupby('msisdn')['phone1_loc_city'].agg(\n",
    "            lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
    "        most_common_county = data.groupby('msisdn')['phone1_loc_province'].agg(\n",
    "            lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
    "        # 每天打电话最早小时数、最晚小时数、每天打电话的个数\n",
    "        data['date'] = data['start_time'].dt.date\n",
    "        daily_stats = data.groupby(['msisdn', 'date']).agg({\n",
    "            'start_hour': ['min', 'max'],\n",
    "            'call_duration': 'count'\n",
    "        })\n",
    "        daily_stats.columns = ['_'.join(col).strip()\n",
    "                               for col in daily_stats.columns.values]\n",
    "        daily_stats.reset_index(inplace=True)\n",
    "\n",
    "        daily_stats_agg = daily_stats.groupby('msisdn').agg({\n",
    "            'start_hour_min': ['mean', 'std'],\n",
    "            'start_hour_max': ['mean', 'std'],\n",
    "            'call_duration_count': ['mean', 'std']\n",
    "        })\n",
    "        daily_stats_agg.columns = [\n",
    "            '_'.join(col).strip() for col in daily_stats_agg.columns.values]\n",
    "\n",
    "        # 合并所有新特征\n",
    "        advanced_features = pd.concat(\n",
    "            [call_stats, most_common_city, most_common_county, daily_stats_agg], axis=1)\n",
    "        advanced_features.columns = [\n",
    "            f\"advanced_{col}\" for col in advanced_features.columns]\n",
    "\n",
    "        return advanced_features\n",
    "\n",
    "    @staticmethod\n",
    "    def _binary_operations(user_aggregated_data):\n",
    "        if 'cfee_sum' in user_aggregated_data.columns and 'lfee_sum' in user_aggregated_data.columns:\n",
    "            user_aggregated_data['cfee_lfee_sum'] = user_aggregated_data['cfee_sum'] + \\\n",
    "                user_aggregated_data['lfee_sum']\n",
    "            user_aggregated_data['cfee_lfee_diff'] = user_aggregated_data['cfee_sum'] - \\\n",
    "                user_aggregated_data['lfee_sum']\n",
    "            user_aggregated_data['cfee_lfee_prod'] = user_aggregated_data['cfee_sum'] * \\\n",
    "                user_aggregated_data['lfee_sum']\n",
    "            user_aggregated_data['cfee_lfee_ratio'] = user_aggregated_data['cfee_sum'] / (\n",
    "                user_aggregated_data['lfee_sum'] + 1e-6)\n",
    "        return user_aggregated_data\n",
    "\n",
    "    def preprocess_and_aggregate(self, data, label_data=None, is_validation=False, fit_columns=None, encoding_config=None):\n",
    "        data = self._convert_time_columns(data)\n",
    "        data = self._extract_time_features(data)\n",
    "        numerical_features = ['call_duration', 'cfee',\n",
    "                              'lfee', 'start_hour', 'start_dayofweek']\n",
    "        numerical_features += [col for col in data.columns if col.startswith(\n",
    "            'date_sin_') or col.startswith('date_cos_')]\n",
    "        differential_features = ['call_duration', 'cfee', 'lfee']\n",
    "        categorical_features = [\n",
    "            'call_event', 'roam_type', 'long_type1', 'ismultimedia',\n",
    "            'is_weekend', 'is_working_hour', 'is_suspect'\n",
    "        ]\n",
    "        location_features = [\n",
    "            'home_area_code', 'visit_area_code', 'called_home_code', 'called_code',\n",
    "            'phone1_loc_city', 'phone1_loc_province', 'phone2_loc_city', 'phone2_loc_province'\n",
    "        ]\n",
    "        suspect_types = {3, 5, 6, 9, 11, 12, 17}\n",
    "        data['is_suspect'] = data['phone1_type'].apply(\n",
    "            lambda x: 1 if x in suspect_types else 0)\n",
    "\n",
    "        data = self._encode_categorical_features(\n",
    "            data, encoding_config, fit=not is_validation)\n",
    "\n",
    "        user_aggregated_data = self._aggregate_numerical_features(\n",
    "            data, numerical_features)\n",
    "        user_aggregated_data = user_aggregated_data.join(\n",
    "            self._aggregate_location_features(data, location_features), how='outer')\n",
    "        user_aggregated_data = self._aggregate_categorical_frequencies(\n",
    "            data, categorical_features, user_aggregated_data)\n",
    "        user_aggregated_data = user_aggregated_data.join(\n",
    "            self._aggregate_differential_features(data, differential_features), how='left')\n",
    "\n",
    "        user_aggregated_data = self._binary_operations(user_aggregated_data)\n",
    "        advanced_features = self._extract_advanced_features(data)\n",
    "        user_aggregated_data = user_aggregated_data.join(\n",
    "            advanced_features, how='left')\n",
    "\n",
    "        user_aggregated_data.fillna(-1, inplace=True)\n",
    "        user_aggregated_data.reset_index(inplace=True)\n",
    "\n",
    "        if not is_validation and label_data is not None:\n",
    "            user_aggregated_data = user_aggregated_data.merge(\n",
    "                label_data, on='msisdn', how='left')\n",
    "\n",
    "        if not is_validation:\n",
    "            numerical_features = [\n",
    "                col for col in user_aggregated_data.columns if col not in ['msisdn', 'is_sa']]\n",
    "            user_aggregated_data[numerical_features] = self.scaler.fit_transform(\n",
    "                user_aggregated_data[numerical_features])\n",
    "            return user_aggregated_data, numerical_features, self.scaler\n",
    "        else:\n",
    "            user_aggregated_data[fit_columns] = self.scaler.transform(\n",
    "                user_aggregated_data[fit_columns])\n",
    "            return user_aggregated_data\n",
    "\n",
    "\n",
    "def feature_selection(train_data, label_column='is_sa', k=20):\n",
    "    X = train_data.drop(columns=['msisdn', label_column])\n",
    "    y = train_data[label_column]\n",
    "\n",
    "    def select_features(model, X, y, k):\n",
    "        model.fit(X, y)\n",
    "        feature_scores = pd.Series(model.feature_importances_, index=X.columns)\n",
    "        return feature_scores.nlargest(k).index.tolist()\n",
    "\n",
    "    xgb_selected_features = select_features(XGBClassifier(n_jobs=-1), X, y, k)\n",
    "    rf_selected_features = select_features(\n",
    "        RandomForestClassifier(n_jobs=-1), X, y, k)\n",
    "    gbdt_selected_features = select_features(\n",
    "        GradientBoostingClassifier(), X, y, k)\n",
    "    et_selected_features = select_features(\n",
    "        ExtraTreesClassifier(n_jobs=-1), X, y, k)\n",
    "\n",
    "    select_k_best = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "    select_k_best.fit(X, y)\n",
    "    mutual_info_selected_features = X.columns[select_k_best.get_support(\n",
    "    )].tolist()\n",
    "\n",
    "    select_k_best = SelectKBest(score_func=f_classif, k=k)\n",
    "    select_k_best.fit(X, y)\n",
    "    f_classif_selected_features = X.columns[select_k_best.get_support(\n",
    "    )].tolist()\n",
    "\n",
    "    rfe_lr = LogisticRegression(random_state=42)\n",
    "    rfe = RFE(rfe_lr, n_features_to_select=k)\n",
    "    rfe.fit(X, y)\n",
    "    rfe_lr_selected_features = X.columns[rfe.support_].tolist()\n",
    "\n",
    "    rfe_svc = LinearSVC()\n",
    "    rfe = RFE(rfe_svc, n_features_to_select=k)\n",
    "    rfe.fit(X, y)\n",
    "    rfe_svc_selected_features = X.columns[rfe.support_].tolist()\n",
    "\n",
    "    corr_coef_selected_features = X.corrwith(\n",
    "        y).abs().nlargest(k).index.tolist()\n",
    "\n",
    "    all_selected_features = (\n",
    "        xgb_selected_features + rf_selected_features + gbdt_selected_features + et_selected_features +\n",
    "        mutual_info_selected_features + f_classif_selected_features +\n",
    "        rfe_lr_selected_features + rfe_svc_selected_features +\n",
    "        corr_coef_selected_features\n",
    "    )\n",
    "\n",
    "    combined_features = pd.Series(\n",
    "        all_selected_features).value_counts().nlargest(k).index.tolist()\n",
    "    return combined_features\n",
    "\n",
    "\n",
    "# Encoding configuration\n",
    "encoding_config = {\n",
    "    'call_event': 'label',\n",
    "    'other_party': 'label',\n",
    "    'ismultimedia': 'label',\n",
    "    'home_area_code': 'frequency',\n",
    "    'visit_area_code': 'frequency',\n",
    "    'called_home_code': 'frequency',\n",
    "    'called_code': 'frequency',\n",
    "    'a_serv_type': 'label',\n",
    "    'long_type1': 'label',\n",
    "    'roam_type': 'label',\n",
    "    'a_product_id': 'tfidf',  # or 'countvector'\n",
    "    'phone1_type': 'tfidf',  # or 'countvector'\n",
    "    'phone2_type': 'tfidf',  # or 'countvector'\n",
    "    'phone1_loc_city': 'tfidf',  # or 'countvector'\n",
    "    'phone1_loc_province': 'tfidf',  # or 'countvector'\n",
    "    'phone2_loc_city': 'tfidf',  # or 'countvector'\n",
    "    'phone2_loc_province': 'tfidf',  # or 'countvector'\n",
    "    'a_product_id': 'count',\n",
    "    'phone1_type': 'label',\n",
    "    'phone2_type': 'label',\n",
    "    'phone1_loc_city': 'frequency',\n",
    "    'phone1_loc_province': 'frequency',\n",
    "    'phone2_loc_city': 'frequency',\n",
    "    'phone2_loc_province': 'frequency',\n",
    "    'is_weekend': 'label',\n",
    "    'is_working_hour': 'label'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "train_set_res = pd.read_csv(\n",
    "    '/home/hwxu/Projects/Competition/Telecom/Input/raw/train.csv', low_memory=False)\n",
    "train_set_ans = pd.read_csv(\n",
    "    '/home/hwxu/Projects/Competition/Telecom/Input/raw/labels.csv', low_memory=False)\n",
    "validation_set_res = pd.read_csv(\n",
    "    '/home/hwxu/Projects/Competition/Telecom/Input/raw/val.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(n_features_to_keep=40):\n",
    "    # 实例化数据处理器\n",
    "    data_processor = DataProcessor()\n",
    "    # 处理训练集\n",
    "    train_data, fit_columns, scaler = data_processor.preprocess_and_aggregate(\n",
    "        train_set_res, train_set_ans, is_validation=False, encoding_config=encoding_config)\n",
    "\n",
    "    print(f\"num features: {train_data.shape[1]}\")\n",
    "    print(f\"Constructed features: {train_data.columns.tolist()}\")\n",
    "\n",
    "    # 选择特征\n",
    "    selected_features = feature_selection(train_data, k=n_features_to_keep)\n",
    "    train_data = train_data[['msisdn'] + selected_features + ['is_sa']]\n",
    "\n",
    "    print(f\"num selected features: {len(selected_features)}\")\n",
    "    print(f\"Selected features: {selected_features}\")\n",
    "    # 处理验证集\n",
    "    validation_data = data_processor.preprocess_and_aggregate(\n",
    "        validation_set_res, is_validation=True, fit_columns=fit_columns, encoding_config=encoding_config)\n",
    "\n",
    "    # 选择特征（根据训练集选择的特征）\n",
    "    validation_data = validation_data[['msisdn'] + selected_features]\n",
    "\n",
    "    # 输出处理后的训练集和验证集\n",
    "    train_data.to_csv(\n",
    "        f'/home/hwxu/Projects/Competition/Telecom/Input/processed/train{n_features_to_keep}.csv', index=False)\n",
    "    validation_data.to_csv(\n",
    "        f'/home/hwxu/Projects/Competition/Telecom/Input/processed/val{n_features_to_keep}.csv', index=False)\n",
    "    print(f'Shape of train data: {train_data.shape}, shape of validation data: {validation_data.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "num features: 319\n",
      "Constructed features: ['msisdn', 'call_duration_sum', 'call_duration_mean', 'call_duration_max', 'call_duration_min', 'call_duration_std', 'call_duration_var', 'call_duration_median', 'call_duration_nunique', 'call_duration_size', 'call_duration_count', 'call_duration_skew', 'call_duration_kurt', 'call_duration_quantile_25', 'call_duration_quantile_75', 'call_duration_mode', 'cfee_sum', 'cfee_mean', 'cfee_max', 'cfee_min', 'cfee_std', 'cfee_var', 'cfee_median', 'cfee_nunique', 'cfee_size', 'cfee_count', 'cfee_skew', 'cfee_kurt', 'cfee_quantile_25', 'cfee_quantile_75', 'cfee_mode', 'lfee_sum', 'lfee_mean', 'lfee_max', 'lfee_min', 'lfee_std', 'lfee_var', 'lfee_median', 'lfee_nunique', 'lfee_size', 'lfee_count', 'lfee_skew', 'lfee_kurt', 'lfee_quantile_25', 'lfee_quantile_75', 'lfee_mode', 'start_hour_sum', 'start_hour_mean', 'start_hour_max', 'start_hour_min', 'start_hour_std', 'start_hour_var', 'start_hour_median', 'start_hour_nunique', 'start_hour_size', 'start_hour_count', 'start_hour_skew', 'start_hour_kurt', 'start_hour_quantile_25', 'start_hour_quantile_75', 'start_hour_mode', 'start_dayofweek_sum', 'start_dayofweek_mean', 'start_dayofweek_max', 'start_dayofweek_min', 'start_dayofweek_std', 'start_dayofweek_var', 'start_dayofweek_median', 'start_dayofweek_nunique', 'start_dayofweek_size', 'start_dayofweek_count', 'start_dayofweek_skew', 'start_dayofweek_kurt', 'start_dayofweek_quantile_25', 'start_dayofweek_quantile_75', 'start_dayofweek_mode', 'date_sin_second_sum', 'date_sin_second_mean', 'date_sin_second_max', 'date_sin_second_min', 'date_sin_second_std', 'date_sin_second_var', 'date_sin_second_median', 'date_sin_second_nunique', 'date_sin_second_size', 'date_sin_second_count', 'date_sin_second_skew', 'date_sin_second_kurt', 'date_sin_second_quantile_25', 'date_sin_second_quantile_75', 'date_sin_second_mode', 'date_cos_second_sum', 'date_cos_second_mean', 'date_cos_second_max', 'date_cos_second_min', 'date_cos_second_std', 'date_cos_second_var', 'date_cos_second_median', 'date_cos_second_nunique', 'date_cos_second_size', 'date_cos_second_count', 'date_cos_second_skew', 'date_cos_second_kurt', 'date_cos_second_quantile_25', 'date_cos_second_quantile_75', 'date_cos_second_mode', 'date_sin_minute_sum', 'date_sin_minute_mean', 'date_sin_minute_max', 'date_sin_minute_min', 'date_sin_minute_std', 'date_sin_minute_var', 'date_sin_minute_median', 'date_sin_minute_nunique', 'date_sin_minute_size', 'date_sin_minute_count', 'date_sin_minute_skew', 'date_sin_minute_kurt', 'date_sin_minute_quantile_25', 'date_sin_minute_quantile_75', 'date_sin_minute_mode', 'date_cos_minute_sum', 'date_cos_minute_mean', 'date_cos_minute_max', 'date_cos_minute_min', 'date_cos_minute_std', 'date_cos_minute_var', 'date_cos_minute_median', 'date_cos_minute_nunique', 'date_cos_minute_size', 'date_cos_minute_count', 'date_cos_minute_skew', 'date_cos_minute_kurt', 'date_cos_minute_quantile_25', 'date_cos_minute_quantile_75', 'date_cos_minute_mode', 'date_sin_hour_sum', 'date_sin_hour_mean', 'date_sin_hour_max', 'date_sin_hour_min', 'date_sin_hour_std', 'date_sin_hour_var', 'date_sin_hour_median', 'date_sin_hour_nunique', 'date_sin_hour_size', 'date_sin_hour_count', 'date_sin_hour_skew', 'date_sin_hour_kurt', 'date_sin_hour_quantile_25', 'date_sin_hour_quantile_75', 'date_sin_hour_mode', 'date_cos_hour_sum', 'date_cos_hour_mean', 'date_cos_hour_max', 'date_cos_hour_min', 'date_cos_hour_std', 'date_cos_hour_var', 'date_cos_hour_median', 'date_cos_hour_nunique', 'date_cos_hour_size', 'date_cos_hour_count', 'date_cos_hour_skew', 'date_cos_hour_kurt', 'date_cos_hour_quantile_25', 'date_cos_hour_quantile_75', 'date_cos_hour_mode', 'date_sin_day_sum', 'date_sin_day_mean', 'date_sin_day_max', 'date_sin_day_min', 'date_sin_day_std', 'date_sin_day_var', 'date_sin_day_median', 'date_sin_day_nunique', 'date_sin_day_size', 'date_sin_day_count', 'date_sin_day_skew', 'date_sin_day_kurt', 'date_sin_day_quantile_25', 'date_sin_day_quantile_75', 'date_sin_day_mode', 'date_cos_day_sum', 'date_cos_day_mean', 'date_cos_day_max', 'date_cos_day_min', 'date_cos_day_std', 'date_cos_day_var', 'date_cos_day_median', 'date_cos_day_nunique', 'date_cos_day_size', 'date_cos_day_count', 'date_cos_day_skew', 'date_cos_day_kurt', 'date_cos_day_quantile_25', 'date_cos_day_quantile_75', 'date_cos_day_mode', 'date_sin_dayofweek_sum', 'date_sin_dayofweek_mean', 'date_sin_dayofweek_max', 'date_sin_dayofweek_min', 'date_sin_dayofweek_std', 'date_sin_dayofweek_var', 'date_sin_dayofweek_median', 'date_sin_dayofweek_nunique', 'date_sin_dayofweek_size', 'date_sin_dayofweek_count', 'date_sin_dayofweek_skew', 'date_sin_dayofweek_kurt', 'date_sin_dayofweek_quantile_25', 'date_sin_dayofweek_quantile_75', 'date_sin_dayofweek_mode', 'date_cos_dayofweek_sum', 'date_cos_dayofweek_mean', 'date_cos_dayofweek_max', 'date_cos_dayofweek_min', 'date_cos_dayofweek_std', 'date_cos_dayofweek_var', 'date_cos_dayofweek_median', 'date_cos_dayofweek_nunique', 'date_cos_dayofweek_size', 'date_cos_dayofweek_count', 'date_cos_dayofweek_skew', 'date_cos_dayofweek_kurt', 'date_cos_dayofweek_quantile_25', 'date_cos_dayofweek_quantile_75', 'date_cos_dayofweek_mode', 'date_sin_month_sum', 'date_sin_month_mean', 'date_sin_month_max', 'date_sin_month_min', 'date_sin_month_std', 'date_sin_month_var', 'date_sin_month_median', 'date_sin_month_nunique', 'date_sin_month_size', 'date_sin_month_count', 'date_sin_month_skew', 'date_sin_month_kurt', 'date_sin_month_quantile_25', 'date_sin_month_quantile_75', 'date_sin_month_mode', 'date_cos_month_sum', 'date_cos_month_mean', 'date_cos_month_max', 'date_cos_month_min', 'date_cos_month_std', 'date_cos_month_var', 'date_cos_month_median', 'date_cos_month_nunique', 'date_cos_month_size', 'date_cos_month_count', 'date_cos_month_skew', 'date_cos_month_kurt', 'date_cos_month_quantile_25', 'date_cos_month_quantile_75', 'date_cos_month_mode', 'home_area_code_nunique', 'home_area_code_count', 'visit_area_code_nunique', 'visit_area_code_count', 'called_home_code_nunique', 'called_home_code_count', 'called_code_nunique', 'called_code_count', 'phone1_loc_city_nunique', 'phone1_loc_city_count', 'phone1_loc_province_nunique', 'phone1_loc_province_count', 'phone2_loc_city_nunique', 'phone2_loc_city_count', 'phone2_loc_province_nunique', 'phone2_loc_province_count', 'call_event_0.0_freq', 'call_event_1.0_freq', 'roam_type_0.0_freq', 'roam_type_1.0_freq', 'roam_type_2.0_freq', 'roam_type_3.0_freq', 'roam_type_4.0_freq', 'roam_type_5.0_freq', 'long_type1_0.0_freq', 'long_type1_1.0_freq', 'long_type1_2.0_freq', 'long_type1_3.0_freq', 'ismultimedia_0.0_freq', 'is_weekend_0.0_freq', 'is_weekend_1.0_freq', 'is_working_hour_0.0_freq', 'is_working_hour_1.0_freq', 'is_suspect_0_freq', 'is_suspect_1_freq', 'call_duration_diff_mean', 'call_duration_diff_std', 'cfee_diff_mean', 'cfee_diff_std', 'lfee_diff_mean', 'lfee_diff_std', 'call_duration_diff2_mean', 'call_duration_diff2_std', 'cfee_diff2_mean', 'cfee_diff2_std', 'lfee_diff2_mean', 'lfee_diff2_std', 'cfee_lfee_sum', 'cfee_lfee_diff', 'cfee_lfee_prod', 'cfee_lfee_ratio', 'advanced_msisdn_count', 'advanced_msisdn_nunique', 'advanced_avg_calls_per_person', 'advanced_phone1_loc_city', 'advanced_phone1_loc_province', 'advanced_start_hour_min_mean', 'advanced_start_hour_min_std', 'advanced_start_hour_max_mean', 'advanced_start_hour_max_std', 'advanced_call_duration_count_mean', 'advanced_call_duration_count_std', 'is_sa']\n",
      "num selected features: 175\n",
      "Selected features: ['start_dayofweek_kurt', 'date_sin_day_max', 'date_sin_second_size', 'is_working_hour_1.0_freq', 'date_cos_day_quantile_75', 'cfee_median', 'date_sin_second_count', 'start_dayofweek_nunique', 'start_hour_sum', 'date_sin_day_quantile_75', 'advanced_call_duration_count_std', 'start_hour_nunique', 'start_hour_quantile_25', 'date_cos_dayofweek_kurt', 'date_sin_dayofweek_kurt', 'start_dayofweek_max', 'date_sin_day_mode', 'start_hour_kurt', 'date_sin_dayofweek_min', 'date_sin_day_min', 'date_sin_day_skew', 'date_sin_day_std', 'advanced_start_hour_max_mean', 'date_cos_day_kurt', 'date_sin_month_sum', 'start_hour_max', 'start_dayofweek_std', 'date_cos_month_sum', 'phone2_loc_province_nunique', 'date_cos_dayofweek_std', 'date_sin_day_kurt', 'advanced_start_hour_min_mean', 'call_duration_sum', 'start_dayofweek_quantile_75', 'called_home_code_nunique', 'start_hour_size', 'cfee_nunique', 'start_dayofweek_min', 'long_type1_0.0_freq', 'cfee_quantile_75', 'is_working_hour_0.0_freq', 'call_duration_count', 'cfee_mode', 'long_type1_2.0_freq', 'date_sin_dayofweek_std', 'call_event_0.0_freq', 'start_hour_var', 'advanced_call_duration_count_mean', 'date_cos_day_nunique', 'date_cos_dayofweek_nunique', 'date_cos_dayofweek_quantile_75', 'start_hour_std', 'date_cos_day_mean', 'start_dayofweek_sum', 'cfee_diff2_std', 'cfee_count', 'start_dayofweek_skew', 'cfee_quantile_25', 'date_sin_month_size', 'cfee_size', 'date_sin_day_mean', 'date_cos_day_median', 'call_duration_size', 'date_cos_dayofweek_size', 'start_hour_count', 'start_dayofweek_size', 'phone2_loc_city_nunique', 'cfee_diff_std', 'date_sin_day_var', 'date_sin_dayofweek_quantile_25', 'lfee_size', 'call_event_1.0_freq', 'call_duration_max', 'date_sin_dayofweek_quantile_75', 'cfee_std', 'date_cos_day_max', 'call_duration_nunique', 'date_sin_dayofweek_max', 'date_cos_day_std', 'lfee_count', 'cfee_lfee_ratio', 'start_hour_min', 'start_dayofweek_var', 'call_duration_quantile_75', 'cfee_max', 'start_dayofweek_count', 'date_cos_day_skew', 'date_cos_dayofweek_sum', 'date_cos_hour_size', 'date_cos_hour_sum', 'date_cos_month_std', 'advanced_phone1_loc_province', 'call_duration_kurt', 'date_sin_day_quantile_25', 'call_duration_mean', 'date_cos_day_size', 'start_dayofweek_quantile_25', 'date_sin_dayofweek_var', 'date_sin_month_count', 'date_sin_dayofweek_nunique', 'date_sin_day_median', 'date_sin_day_nunique', 'advanced_start_hour_max_std', 'date_sin_month_std', 'date_cos_dayofweek_count', 'called_code_count', 'date_cos_day_min', 'cfee_lfee_sum', 'date_cos_month_count', 'date_sin_day_sum', 'date_sin_dayofweek_size', 'date_cos_dayofweek_max', 'call_duration_skew', 'start_hour_quantile_75', 'date_cos_dayofweek_skew', 'advanced_start_hour_min_std', 'date_cos_second_size', 'advanced_avg_calls_per_person', 'called_code_nunique', 'date_cos_month_skew', 'call_duration_diff2_mean', 'date_cos_day_sum', 'date_cos_second_sum', 'cfee_mean', 'date_cos_month_size', 'phone2_loc_province_count', 'date_sin_second_std', 'date_sin_second_var', 'date_sin_dayofweek_count', 'visit_area_code_count', 'date_cos_second_count', 'date_cos_minute_sum', 'date_cos_day_count', 'date_cos_hour_count', 'date_sin_minute_count', 'advanced_msisdn_count', 'date_sin_hour_count', 'phone1_loc_province_count', 'date_cos_dayofweek_var', 'date_cos_minute_count', 'date_sin_day_size', 'date_cos_minute_size', 'phone2_loc_city_count', 'date_cos_day_var', 'date_cos_month_mean', 'called_home_code_count', 'start_dayofweek_median', 'date_cos_day_mode', 'cfee_sum', 'call_duration_std', 'start_hour_mean', 'visit_area_code_nunique', 'call_duration_diff2_std', 'call_duration_diff_std', 'start_dayofweek_mean', 'date_sin_dayofweek_mean', 'call_duration_min', 'cfee_skew', 'roam_type_0.0_freq', 'date_sin_dayofweek_mode', 'date_cos_day_quantile_25', 'date_cos_dayofweek_median', 'roam_type_2.0_freq', 'call_duration_quantile_25', 'date_sin_dayofweek_sum', 'date_sin_month_mean', 'date_sin_month_kurt', 'long_type1_1.0_freq', 'start_hour_median', 'date_cos_dayofweek_quantile_25', 'home_area_code_count', 'date_sin_month_nunique', 'date_sin_dayofweek_median', 'call_duration_diff_mean', 'start_hour_skew']\n",
      "Shape of train data: (3836, 177), shape of validation data: (1278, 176)\n"
     ]
    }
   ],
   "source": [
    "main(n_features_to_keep=175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "num features: 318\n",
      "Constructed features: ['msisdn', 'call_duration_sum', 'call_duration_mean', 'call_duration_max', 'call_duration_min', 'call_duration_std', 'call_duration_var', 'call_duration_median', 'call_duration_nunique', 'call_duration_size', 'call_duration_count', 'call_duration_skew', 'call_duration_kurt', 'call_duration_quantile_25', 'call_duration_quantile_75', 'call_duration_mode', 'cfee_sum', 'cfee_mean', 'cfee_max', 'cfee_min', 'cfee_std', 'cfee_var', 'cfee_median', 'cfee_nunique', 'cfee_size', 'cfee_count', 'cfee_skew', 'cfee_kurt', 'cfee_quantile_25', 'cfee_quantile_75', 'cfee_mode', 'lfee_sum', 'lfee_mean', 'lfee_max', 'lfee_min', 'lfee_std', 'lfee_var', 'lfee_median', 'lfee_nunique', 'lfee_size', 'lfee_count', 'lfee_skew', 'lfee_kurt', 'lfee_quantile_25', 'lfee_quantile_75', 'lfee_mode', 'start_hour_sum', 'start_hour_mean', 'start_hour_max', 'start_hour_min', 'start_hour_std', 'start_hour_var', 'start_hour_median', 'start_hour_nunique', 'start_hour_size', 'start_hour_count', 'start_hour_skew', 'start_hour_kurt', 'start_hour_quantile_25', 'start_hour_quantile_75', 'start_hour_mode', 'start_dayofweek_sum', 'start_dayofweek_mean', 'start_dayofweek_max', 'start_dayofweek_min', 'start_dayofweek_std', 'start_dayofweek_var', 'start_dayofweek_median', 'start_dayofweek_nunique', 'start_dayofweek_size', 'start_dayofweek_count', 'start_dayofweek_skew', 'start_dayofweek_kurt', 'start_dayofweek_quantile_25', 'start_dayofweek_quantile_75', 'start_dayofweek_mode', 'date_sin_second_sum', 'date_sin_second_mean', 'date_sin_second_max', 'date_sin_second_min', 'date_sin_second_std', 'date_sin_second_var', 'date_sin_second_median', 'date_sin_second_nunique', 'date_sin_second_size', 'date_sin_second_count', 'date_sin_second_skew', 'date_sin_second_kurt', 'date_sin_second_quantile_25', 'date_sin_second_quantile_75', 'date_sin_second_mode', 'date_cos_second_sum', 'date_cos_second_mean', 'date_cos_second_max', 'date_cos_second_min', 'date_cos_second_std', 'date_cos_second_var', 'date_cos_second_median', 'date_cos_second_nunique', 'date_cos_second_size', 'date_cos_second_count', 'date_cos_second_skew', 'date_cos_second_kurt', 'date_cos_second_quantile_25', 'date_cos_second_quantile_75', 'date_cos_second_mode', 'date_sin_minute_sum', 'date_sin_minute_mean', 'date_sin_minute_max', 'date_sin_minute_min', 'date_sin_minute_std', 'date_sin_minute_var', 'date_sin_minute_median', 'date_sin_minute_nunique', 'date_sin_minute_size', 'date_sin_minute_count', 'date_sin_minute_skew', 'date_sin_minute_kurt', 'date_sin_minute_quantile_25', 'date_sin_minute_quantile_75', 'date_sin_minute_mode', 'date_cos_minute_sum', 'date_cos_minute_mean', 'date_cos_minute_max', 'date_cos_minute_min', 'date_cos_minute_std', 'date_cos_minute_var', 'date_cos_minute_median', 'date_cos_minute_nunique', 'date_cos_minute_size', 'date_cos_minute_count', 'date_cos_minute_skew', 'date_cos_minute_kurt', 'date_cos_minute_quantile_25', 'date_cos_minute_quantile_75', 'date_cos_minute_mode', 'date_sin_hour_sum', 'date_sin_hour_mean', 'date_sin_hour_max', 'date_sin_hour_min', 'date_sin_hour_std', 'date_sin_hour_var', 'date_sin_hour_median', 'date_sin_hour_nunique', 'date_sin_hour_size', 'date_sin_hour_count', 'date_sin_hour_skew', 'date_sin_hour_kurt', 'date_sin_hour_quantile_25', 'date_sin_hour_quantile_75', 'date_sin_hour_mode', 'date_cos_hour_sum', 'date_cos_hour_mean', 'date_cos_hour_max', 'date_cos_hour_min', 'date_cos_hour_std', 'date_cos_hour_var', 'date_cos_hour_median', 'date_cos_hour_nunique', 'date_cos_hour_size', 'date_cos_hour_count', 'date_cos_hour_skew', 'date_cos_hour_kurt', 'date_cos_hour_quantile_25', 'date_cos_hour_quantile_75', 'date_cos_hour_mode', 'date_sin_day_sum', 'date_sin_day_mean', 'date_sin_day_max', 'date_sin_day_min', 'date_sin_day_std', 'date_sin_day_var', 'date_sin_day_median', 'date_sin_day_nunique', 'date_sin_day_size', 'date_sin_day_count', 'date_sin_day_skew', 'date_sin_day_kurt', 'date_sin_day_quantile_25', 'date_sin_day_quantile_75', 'date_sin_day_mode', 'date_cos_day_sum', 'date_cos_day_mean', 'date_cos_day_max', 'date_cos_day_min', 'date_cos_day_std', 'date_cos_day_var', 'date_cos_day_median', 'date_cos_day_nunique', 'date_cos_day_size', 'date_cos_day_count', 'date_cos_day_skew', 'date_cos_day_kurt', 'date_cos_day_quantile_25', 'date_cos_day_quantile_75', 'date_cos_day_mode', 'date_sin_dayofweek_sum', 'date_sin_dayofweek_mean', 'date_sin_dayofweek_max', 'date_sin_dayofweek_min', 'date_sin_dayofweek_std', 'date_sin_dayofweek_var', 'date_sin_dayofweek_median', 'date_sin_dayofweek_nunique', 'date_sin_dayofweek_size', 'date_sin_dayofweek_count', 'date_sin_dayofweek_skew', 'date_sin_dayofweek_kurt', 'date_sin_dayofweek_quantile_25', 'date_sin_dayofweek_quantile_75', 'date_sin_dayofweek_mode', 'date_cos_dayofweek_sum', 'date_cos_dayofweek_mean', 'date_cos_dayofweek_max', 'date_cos_dayofweek_min', 'date_cos_dayofweek_std', 'date_cos_dayofweek_var', 'date_cos_dayofweek_median', 'date_cos_dayofweek_nunique', 'date_cos_dayofweek_size', 'date_cos_dayofweek_count', 'date_cos_dayofweek_skew', 'date_cos_dayofweek_kurt', 'date_cos_dayofweek_quantile_25', 'date_cos_dayofweek_quantile_75', 'date_cos_dayofweek_mode', 'date_sin_month_sum', 'date_sin_month_mean', 'date_sin_month_max', 'date_sin_month_min', 'date_sin_month_std', 'date_sin_month_var', 'date_sin_month_median', 'date_sin_month_nunique', 'date_sin_month_size', 'date_sin_month_count', 'date_sin_month_skew', 'date_sin_month_kurt', 'date_sin_month_quantile_25', 'date_sin_month_quantile_75', 'date_sin_month_mode', 'date_cos_month_sum', 'date_cos_month_mean', 'date_cos_month_max', 'date_cos_month_min', 'date_cos_month_std', 'date_cos_month_var', 'date_cos_month_median', 'date_cos_month_nunique', 'date_cos_month_size', 'date_cos_month_count', 'date_cos_month_skew', 'date_cos_month_kurt', 'date_cos_month_quantile_25', 'date_cos_month_quantile_75', 'date_cos_month_mode', 'home_area_code_nunique', 'home_area_code_count', 'visit_area_code_nunique', 'visit_area_code_count', 'called_home_code_nunique', 'called_home_code_count', 'called_code_nunique', 'called_code_count', 'phone1_loc_city_nunique', 'phone1_loc_city_count', 'phone1_loc_province_nunique', 'phone1_loc_province_count', 'phone2_loc_city_nunique', 'phone2_loc_city_count', 'phone2_loc_province_nunique', 'phone2_loc_province_count', 'call_event_0.0_freq', 'call_event_1.0_freq', 'roam_type_0.0_freq', 'roam_type_1.0_freq', 'roam_type_2.0_freq', 'roam_type_3.0_freq', 'roam_type_4.0_freq', 'roam_type_5.0_freq', 'long_type1_0.0_freq', 'long_type1_1.0_freq', 'long_type1_2.0_freq', 'long_type1_3.0_freq', 'ismultimedia_0.0_freq', 'is_weekend_0.0_freq', 'is_weekend_1.0_freq', 'is_working_hour_0.0_freq', 'is_working_hour_1.0_freq', 'is_suspect_0_freq', 'call_duration_diff_mean', 'call_duration_diff_std', 'cfee_diff_mean', 'cfee_diff_std', 'lfee_diff_mean', 'lfee_diff_std', 'call_duration_diff2_mean', 'call_duration_diff2_std', 'cfee_diff2_mean', 'cfee_diff2_std', 'lfee_diff2_mean', 'lfee_diff2_std', 'cfee_lfee_sum', 'cfee_lfee_diff', 'cfee_lfee_prod', 'cfee_lfee_ratio', 'advanced_msisdn_count', 'advanced_msisdn_nunique', 'advanced_avg_calls_per_person', 'advanced_phone1_loc_city', 'advanced_phone1_loc_province', 'advanced_start_hour_min_mean', 'advanced_start_hour_min_std', 'advanced_start_hour_max_mean', 'advanced_start_hour_max_std', 'advanced_call_duration_count_mean', 'advanced_call_duration_count_std', 'is_sa']\n"
     ]
    }
   ],
   "source": [
    "main(n_features_to_keep=180)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hwxu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
